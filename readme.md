# Команда 3 #
Для сохранения наших персональных данных, ссылки и таблички в TG, смотрим закреп!

## Структура проекта ##
Flood Prediction/  
data/ # Данные  
    raw/ # Исходные данные (не изменять!)  
    processed/ # Предобработанные данные  
notebooks/ # Jupyter ноутбуки  
      01_eda/ # Разведочный анализ  
    02_preprocessing/ # Предобработка  
    03_modeling/ # Моделирование  
src/ # Исходный код  
    data/ # Скрипты работы с данными  
    features/ # Создание признаков  
    models/ # ML модели  
reports/ # Отчеты и графики  
    figures/ # Сохраненные визуализации  
docs/ # Документация, словари  
requirements.txt # Зависимости проекта
readme.md
 
Готовимся к работе:
1. Скачиваем python 3.11 https://www.python.org/downloads/release/python-3119/
2. Устанавливаем с галочкой add PATH в первом окне
3. В зависимости от вашего IDE создаем вирутальное окружение под наш проект, активируем его
4. В терминале нашего IDE устанавливаем библиотеки для проект (будут дополняться, после открытия новых тем на практиках):   
Базовый пакет:   
pip install numpy pandas matplotlib scikit-learn tensorflow torch jupyter seaborn plotly xgboost lightgbm catboost  
Интересный пакет после первой пары:  
pip install -U ydata-profiling  

## Формат взаимодействия ##
Основное поле коммуникации ТГ чатик, раз в неделю онлайн звонок на 15-20 минут.

## Касательно нашего репо!!! ##
1. У нас большая группа, мы все новички. Во избежании мусорки в репо делаем на каждый спринт отдельный бранч
2. САМОЕ ВАЖНОЕ И ГЛАВНОЕ!!! Чтобы каждый участник смог научиться и сохранить практику ОБЯЗАТЕЛЬНО ДЕЛАЕМ Pull Request (Merge Request) - если дата аналитик, что пушит, то второй дата аналитик проверяет, если МЛ пушит, второй МЛ обязательно проверяет!

## Спринт 1 - Линейная регрессия ##
disclaimer пробуем использовать терминалогию и лексикон АйТи, в качестве наших идентификаторов используем предложенные сокращения, я - МШ.

Отчетная задача к паре в субботу:  
1. Загружаем открываем данные.  
Задача для всей команды. Скачиваем с каггле (продублировал дата сет в ТГ нашем)  
Открываем cvs файлы, смотрим, читаем словарь из ТГ канала.  
Ноутбуки пожалуйста в notebooks/01_eda/

2. Провести анализ данных (развед анализ)  
Задача в первую очередь для роли Дата аналитик, остальные как суппорт могут повторить действия дата аналитиков, либо не повторять, а сконцентрироваться на своей задаче.  
Данные открываем и анализируем с помощью чудо-пакета ydata-profiling и изучаем результат.  

Дата аналитики на выходе в папку   notebooks/01_eda/   пушат свой юпитер нотебку или отчет.md с ответатами на следующие вопросы:  
* Данные нормальзованы? Есть ли пропуски (пустые строчки, столбцы)? Найшли ли выбросы?
* Распределение нормальное?
* какое наше целевое значение? и что с ним сильнее всего коррелирует?

3. Подготовка данных для модели  
В первую очередь задача для ML инженеров. Остальные как саппорт могут повторить. На выходе в папку   notebooks/02_preprocessing/   пушите свой юпитер нотебку или отчет.md с ответатами на следующие вопросы:  
* Если аналитики нашли пропуски и выбросы, что делаем? Удаляем? Заполняем медианой? Берем среднюю? В целом все как на парах Наталии Мальцевой делаем по порядку, как в домашках. 
* Всего ли хватает в предложенных данных для модели? Есть запрос дата аналитикам и тим лиду на поиск дополнительных атрибутов, данных?
* То, как данные предложил кагл в трейн и тест - ок? Или пересоберем сами?

4. Бейзлайн модели  
Основная задача ML инженеров.
Пушим в notebooks/03_modeling/:
* Метрики: RMSE, MAE, R²
* Обучить LinearRegression из sklearn на нашем датасете train
* Проверяем по метрикам на test
* Строим на графики - даем оценку - работает модель или не очень.

5. Отчет преподователю  
Делает тимлид по предложенному шаблону:
* Цель анализа	Какую зависимость вы моделируете?
* Зависимая переменная (target)
* Независимые переменные (features)
* Метод построения модели	(например: LinearRegression в sklearn)
* Проверка качества модели (метрики)	MSE, R² и т. д.

Основные выводы	Что показала модель, насколько предсказания разумны?


To be continue...

